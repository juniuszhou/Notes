WhyStable.txt

随机森林背后的主要思想是聚合模型（Ensemble Model）。为什么聚合模型效果好于单一模型（理论推导，请参考附录【聚合模型错误评估】）？直观的理解，当很多模型进行投票时，有一些模型会犯错，另外一些模型正确，那么正确的投票会与错误的投票抵消，整体上只要最终正确的投票多于错误的投票，哪怕多一票，那么就会得到正确的结果。由于相互抵消，所以聚合效果比单一模型稳定。聚合模型中，需要模型间具有较大差异，这样才能覆盖数据的不同方面，这也是为什么随机森林在数据的行和列两个维度上，添加随机过程，用于增大模型之间的差异。



集成学习方法是机器学习领域中用来提升分类算法准确率的技术，主要包括Bagging和Boosting即装袋和提升。


1 booststraping：意思是依靠你自己的资源，称为自助法，它是一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。
其核心思想和基本步骤如下：
（1）采用重抽样技术从原始样本中抽取一定数量（自己给定）的样本，此过程允许重复抽样。
（2）根据抽出的样本计算统计量T。
（3）重复上述N次（一般大于1000），得到统计量T。
（4）计算上述N个统计量T的样本方差，得到统计量的方差。
应该说是Bootstrap是现代统计学较为流行的方法，小样本效果好，通过方差的估计可以构造置信区间等。


Bagging即套袋法，bootstrap aggregating的缩写。其算法过程如下：

A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）

B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

 

2、Boosting

其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。

关于Boosting的两个核心问题：

1）在每一轮如何改变训练数据的权值或概率分布？

通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

2）通过什么方式来组合弱分类器？

通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。

而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

 
 boosting has different implementation such as adaboost and gradient boost.


3、Bagging，Boosting二者之间的区别

Bagging和Boosting的区别：

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。


Rand Forest：随机森林，使用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一课决策树之间是没有关联的。在得到森林之后，当有一个新的输入
样本进入，就让森林中的每一颗决策树分别进行判断，看看这个样本属于那个类，然后看看哪一类被选择多，就预测为那一类。
在建立决策树的过程中，需要注意两点-采样和完全分裂。首先是两个随机采样的过程，random forest对输入的数据要经行行，列的采样。
对于行采样，采用有回放的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为n个。这样使得在训练的时候，每一颗树的
输入样本都不是全部样本，使得相对不容易出现over-fitting。然后进行采样，从M个feature中，选择m个。
之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂，要么里面的所有样本都是指向的同一类分类。一般的决策树
都有一个重要的步骤，剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会over-fitting。按这种算法得到的随机森林中的每一颗
树都是很弱的，但是大家组合起来就很厉害了。可以这样比喻随机森林：每一颗决策树就是一个精通于某一个窄领域的专家，这样在随机森林中就有了很多个精通不同领域的专
家，对于新的样本，可以用不同的角度看待它，最终由各个专家，投票得到结果。



#
为什么说bagging是减少variance，而boosting是减少bias?


Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均。由于子样本集的相似性以及使用的是同种模型，因此各模型有近似相等的bias和variance（事实上，各模型的分布也近似相同，但不独立）。由于，所以bagging后的bias和单个子模型的接近，一般来说不能显著降低bias。另一方面，若各子模型独立，则有，此时可以显著降低variance。若各子模型完全相同，则，此时不会降低variance。
bagging方法得到的各子模型是有一定相关性的，属于上面两个极端状况的中间态，因此可以一定程度降低variance。

为了进一步降低variance，Random forest通过随机选取变量子集做拟合的方式de-correlated了各子模型（树），使得variance进一步降低。（用公式可以一目了然：设有i.d.的n个随机变量，方差记为，两两变量之间的相关性为，则的方差为，bagging降低的是第二项，random forest是同时降低两项。详见ESL p588公式15.1）

boosting从优化角度来看，是用forward-stagewise这种贪心法去最小化损失函数。例如，常见的AdaBoost即等价于用这种方法最小化exponential loss：。所谓forward-stagewise，就是在迭代的第n步，求解新的子模型f(x)及步长a（或者叫组合系数），来最小化，这里是前n-1步得到的子模型的和。因此boosting是在sequential地最小化损失函数，其bias自然逐步下降。但由于是采取这种sequential、adaptive的策略，各子模型之间是强相关的，于是子模型之和并不能显著降低variance。所以说boosting主要还是靠降低bias来提升预测精度。

（2017-3-8更新：此段存疑）另外，计算角度来看，两种方法都可以并行。bagging, random forest并行化方法显而意见。boosting有强力工具stochastic gradient boosting，其本质等价于sgd，并行化方法参考async sgd之类的业界常用方法即可。




## gradient boosting
update each sample's weight via gradient way. 
gradient depends on the choose of loss function.
if least square loss, then gradient is the gap between real output and prediction.
if L1 loss, then gradient just a sign.
if exponential loss, then become adaboost.

4) AdaBoost特性分析
特性1：
    训练误差的上界，随着迭代次数的增加，会逐渐下降。
 
特性2：
    AdaBoost算法即使训练次数很多，也不会出现过度拟合(over fitting)的问题。
    
# for boosting, each weak regressioner or classifer must be tree. why
it is simple, we can control tree as simple via set the man lenght of tree ?

