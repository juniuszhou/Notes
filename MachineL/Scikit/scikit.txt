## install
http://scikit-learn.org/stable/tutorial/basic/tutorial.html 

# knn
在sci-kit中的实现input可以是numpy array也可以使scipy sparse。


instance-based or non-generalizing learning, 就是说它要根据所有的点才能做出判断。而且对于新的数据，方法也一样，并不能形成一个 generalize 一个模型来判断
新数据的分类或者output 在regression中。

for regression, the predict is the mean of all k close neighbor.

Brute-force / radius / ball tree  / k-d tree
这几种方法有时候可以结合来使用，比如对整体数据使用tree，到了某个tree的leaf
数据量不大的时候用brute force.

leaf size parameter.


3.4.2
KNN 有二种方法，一个是把所以得数据都用来计算距离，vote，然后得到最有
可能的class。第二种，是先画个半径，在半径内的点才参与到计算和vote。

还有一个变种是，对于最基本的KNN，所以点vote的weight是一样的，我们可以
使用另外一个变种是，weight和距离成反比。

在实现的时候，找到方差最大的维，然后平分会为以后的查找带来方便。


balltrees are allowed to intersect and need not partition the entire space.

five algorithm for create a ball trees.
1. k-d construction. O(NlogN)
2. top-down O(N(logN)^2)
3. on-line insert 
4. cheaper on-line
5. bottom up (ON^3) and improved bottom up.




nearest centroid classifier: just classification.
it is similar to k-means, every class has a centroid point. 
there is a parameter shrink threshold, which used to remove 
the some feature. 

meshgrid 是k-d tree在numpy中的实现。


k-d tree and ball tree are binary tree.


PCA  principal component analysis

将M维的数据投射到N维，N>>M. 选取特征值最大的。用它的特征向量作为新的坐标轴。
它的理论基础是最大分差理论和最小错误，最小平方误差理论。

 很大的优点是，它是完全无参数限制的。  对符合高斯分布的样本点比较有效
PCR 使用了PCA之后然后在做regression。

ICA independent component analysis
一个例子是声音的分解，音频是一个有些周期性的信号，它不会是高斯分布。

我们有N个人在说话，有M的录入设备，录入设备的数据肯定是包含所以N个人的信号。
ICA其实假设N个人实际的声音是M个数据的线性组合，而且线性组合参数W在整个过程
中没有发生变化。假设录入设备的地点没有移动。
不合理的地方在于N个说话的人可能会走到。

sigmoid函数，


CCA canonical correlation analysis

LDA 线性判别分析  linear discriminant analysis
在classification的问题中，我们除了有数据外，还有label。这个时候我们如果简单的用PCA
来reduce dimension，label就浪费了。我们可以结合label来降维。在选择投射向量的时候
我们尽可能的让二个class mean的距离最大，而class内部的variance最小。
也就是说class内部越紧密，class之间越分得开就越好。
这样的reduce dimension可以使得为以后的分类提到准确性。

 LDA不适合对非高斯分布样本进行降维
 LDA在样本分类信息依赖方差而不是均值时，效果不好 

 
 多类分类(multiclass classification)学习的分类器旨在对一个新的实例指定唯一的分类类别，常用的策略有两类：基于后验概率或距离一次给出所有类别的度量，选择 度量值最大的类别作为预测类别；将多类分类分解为许多二元分类问题，然后组合所有二元分类的结果。
多标签分类(multilabel classification)分类器给一个新的实例指定多个类别。这个分类模型有很广泛的实际应用，如：一个文档可能同时属于多个分类；一个蛋白质可能 具有多个功能。并且，多个标签之间可能存在一定的依赖或约束关系，如蛋白质的所有功能组成的GO(gene ontology)。这个依赖或约束关系具有层次特性，经常可以描述为树或有向无环图结构，机器学习社团称之为层次多标签分类。由于模型的输出具有层次结 构，因此层次多标签分类又属于另外一个近来非常活跃的研究领域：结构预测。层次多标签分类和结构预测都是崭新的、富有挑战性的研究领域。

one vs the rest
one vs one



assume the data generated from several guassian model.

GMM classifier implement the EM algorithm

spherical 分布是球形，
Diagonal 分布是椭圆，并且平行于坐标轴
full  分布可以使任意
tied 应该是可以更加随意，还没有知道和full的区别。

算法快。
如果有一个类，点很少，那么求covariance会非常困难。

DPGMM Dirichlet Process GMM
 Dirichlet Process， don't need to specify the number of compoment.
对分类的个数不敏感，但是算法要慢些。



VBGMM variational inference

 增加了一个concentration rate 的参数，聚合度，是在DPGMM的基础上的。
对于数据量小的情况下，比GMM要好。



狄利克雷过程（dirichlet process ）的五种理解
下面我们谈谈dirichlet process的五种角度来理解它。
第一种：原始定义：假设存在在度量空间\Theta上的分布H和一个参数\alpha，如果对于度量空间\Theta的任意一个可数划分（可以是有限或者无限的）A1, A2,...,An，都有下列式子成立：
(G(A1),G(A2),...,G(An)) ~ Dir(\alpha H(A1), \alpha H(A2),..., \alpha H(An)),  这里Dir是dirichlet 分布，
我们称G是满足Dirichlet process的。
这个定义是1973年Ferguson最早提出的定义。在有了这个定义之后，我们怎么去构造一个dirichlet process（DP）出来呢？或者如果我们想从这个DP中抽取出一些样本，怎么抽呢？由于这个原因，我们有了下面三种构造性定义或者解释： 中国餐馆过程（CRP)，polya urn ，stick-breaking。
第二种：中国餐馆过程（CRP）
假设一个中国餐馆有无限的桌子，第一个顾客到来之后坐在第一张桌子上。第二个顾客来到可以选择坐在第一张桌子上，也可以选择坐在一张新的桌子上，假 设第n＋1个顾客到来的时候，已经有k张桌子上有顾客了，分别坐了n1,n2,...，nk个顾客，那么第n＋1个顾客可以以概率为 ni／(\alpha+n)坐在第i张桌子上，ni为第i张桌子上的顾客数；同时有概率为\alpha／(\alpha+n)选取一张新的桌子坐下。那么 在n个顾客坐定之后，很显然CRP把这n个顾客分为了K个堆，即K个clusters，可以证明CRP就是一个DP。
注意这里有一个限制，每张桌子上只能有同一个dish，即一桌人喜欢吃同一道菜。
第三种：Polya urn模型
假设我们有一个缸，里面没有球，现在我们从一个分布H中选取一种颜色，然后把这种颜色涂在一个球上放入缸中；然后我们要么从缸中抽取一个球出来，然 后再放入两个和这个球同种颜色的球进入缸中；要么就从分布H中选取一个颜色，然后把这种颜色涂在一个球上放入缸中。从缸中抽取某种颜色的一个球的概率是 ni/(\alpha+n)，ni是这种颜色的球的个数，n是总的球个数；不从缸中抽取而放入一种颜色的球的概率是\alpha/(\alpha+n)。 很明显，polya urn模型和CRP有一一对应的关系，颜色对应一个桌子，坐新桌子对应于不从缸中选取而是从H中选取一种颜色涂球放入缸中。
第四种：stick-breaking模型
假设有一个长度为1的线段，我们从中选取\pi_1长出来，剩下的部分再选取\pi_2出来，循环下去，\pi_n，无穷下去，这个有点类似我们古代的一句话：
“一尺之踵，日取其半，万世不竭”，它们满足\sum \pi_i ＝ 1

对每个\pi_i，我们都从分布H中选取一个\theta_i，然后从F(\theta_i)中选取出一个x_i出来。这里的\theta_i就对应一个cluster，类似地，我们可以看到数据自然地被分为了各个堆，可以证明这个模型仍然是一个DP。
第五种：无限混合模型
从stick－breaking模型我们看出，我们可以把DP看着是一个无限混合模型，即
G ~ \sum_1^\inf \pi_i*F(\theta_i)，其中\sum \pi_i ＝ 1。\pi_i 就是混合模型中每个子模型的权重。
目前应用最多的还是从第五种角度来看待问题，即把DP看着是一个无限混合模型，其中值得注意的是：
1）虽然DP是一个无限混合模型，但是可以证明，随着数据的增多，模型的个数是呈现log 增加的，即模型的个数的增长是比数据的增长要缓慢得多的；
2）DP是有一个马太效应在里面的，即越富裕的人越来越富裕，我们可以从第二和第三种解释中看到，每个桌子或者颜色已经有的数据越多，那么下一次被选中的概率越大，因为是与在桌子上的个数成正比的。
DP是一个复杂的随机过程，需要进一步深入理解，下篇将会继续这个话题



6.1.2 normalization 是把每个sample 的norm归一化。
L1或者是L2，就是(x1,x2,x3)使得 
x1+x2+x3 = 1 
or x1^2 + x2^2 + x3^2 = 1

binarization
设置一个数值feature的threshold找到，使得容易得到一个bool值。
就像在logistic中的0.5

label binarization
把一个multi class取值的feature，转换成一个vector。
这个处理在multi class的classification问题中需要。

Whitening的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所 以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。数据的whitening必须满足两个条件：一是不同特征间相关性最小，接近0；二是所有特 征的方差相等（不一定为1）。常见的白化操作有PCA whitening和ZCA whitening。
　　PCA whitening是指将数据x经过PCA降维为z后，可以看出z中每一维是独立的，满足whitening白化的第一个条件，这是只需要将z中的每一维都除以标准差就得到了每一维的方差为1，也就是说方差相等。公式为：
　　
　　ZCA whitening是指数据x先经过PCA变换为z，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足whtienning的第一个条件，特征间相互独立。然后同样进行方差为1的操作，最后将得到的矩阵左乘一个特征向量矩阵U即可。
　　ZCA whitening公式为：
　　

在sci-kit中有这样的功能，但还不清楚具体算法。


bootstrps bagging boosting这几个概念经常用到，现仔细学习了一下：
他们都属于集成学习方法，(如:Bagging，Boosting，Stacking)，将训练的学习器集成在一起,原理来源于PAC学习模型（Probably Approximately CorrectK）。Kearns和Valiant指出，在PAC学习模型中，若存在一个 多项式级的学习算法来识别一组概念，并且识别正确率很高，那么这组概念是强可学习的；而如果学习算法识别一组概念的正确率仅比随机猜测略好，那么这组概念 是弱可学习的。他们提出了弱学习算法与强学习算法的等价性问题，即是否可以将弱学习算法提升成强学习算法。如果两者等价，那么在学习概念时，只要找到一个 比随机猜测略好的弱学习算法，就可以将其提升为强学习算法，而不必直接去找通常情况下很难获得的强学习算法。
bootstraps:名字来自成语“pull up by your own bootstraps”，意思是依靠你自己的资源，它是一种有放回的抽样方法，学习中还发现有种叫jackknife的方法，它是每一次移除一个样本。
bagging: bootstrap aggregating的缩写。让该学习算法训练多轮，每轮的训练集由从初始的训练集中随机取出的n个训练倒组成，初始训练例在某轮训练集中可以出现多次 或根本不出现训练之后可得到一个预测函数序列h．，? ?h 最终的预测函数H对分类问题采用投票方式，对回归问题采用简单平均方法对新示例进行判别。
–(训练R个分类器fi，分类器之间其他相同就是参数不同。其中fi是通过从训练集合中(N篇文档)随机取(取后放回)N次文档构成的训练集合训练得到的。 –对于新文档d，用这R个分类器去分类，得到的最多的那个类别作为d的最终类别.)
boosting:其 中主要的是AdaBoost（Adaptive Boosting）。初始化时对每一个训练例赋相等的权重1／n，然后用该学算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就 是让学习算法在后续的学习中集中对比较难的训练铡进行学习，从而得到一个预测函数序列h 一?h其中h．也有一定的权重，预测效果好的预测函数权重较大，反之较小。最终的预测函数H对分类问题采用有权重的投票方式，对回归问题采用加权平均的方 法对新示例进行判别。( 类似Bagging方法，但是训练是串行进行的，第k个分类器训练时关注对前k-1分类器中错分的文档，即不是随机取，而是加大取这些文档的概率).
Bagging与Boosting的区别：在 于Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的训练集的选择是独立的，各轮训练集的选择与前面各轮的学习结果有 关；Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函 数只能顺序生成。对于象神经网络这样极为耗时的学习方法。Bagging可通过并行训练节省大量时间开销。　　　bagging和boosting都可以 有效地提高分类的准确性。在大多数数据集中，boosting的准确性比bagging高。在有些数据集中，boosting会引起退化。 ---Overfit
文本分类中使用的投票方法（Voting，也叫组合分类器）就是一种典型的集成机器学习方法。它通过组合多个弱分类器来得到一个强分类器，包括Bagging和Boosting两种方式，二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。投票分类方法虽然分类精度较高，但训练时间较长。Boosting思想的一种改进型AdaBoost方法在邮件过滤、文本分类方面都有很好的性能。

下面是随机森林的构造过程：
　　1. 假如有N个样本，则有放回的随机选择N个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
　　2. 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
　　3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
　　4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。
　　从上面的步骤可以看出，随机森林的随机性体现在每颗数的训练样本是随机的，树中每个节点的分类属性也是随机选择的。有了这2个随机的保证，随机森林就不会产生过拟合的现象了。
 
　　随机森林有2个参数需要人为控制，一个是森林中树的数量，一般建议取很大。另一个是m的大小，推荐m的值为M的均方根。
