## db and table level

# delimit critical important other wise load data will fail.
create table students(id INT, name STRING, age INT) row format delimited fields terminated by ',' stored as textfile;

//show all databases.
show schemas;

//show tables in database.
show tables;

describe table1;


表添加一列 ：
hive> ALTER TABLE pokes ADD COLUMNS (new_col INT);
添加一列并增加列字段注释
hive> ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment');
更改表名：
hive> ALTER TABLE events RENAME TO 3koobecaf;
删除列：
hive> DROP TABLE pokes;

## import data     it
load data local inpath '/home/students.txt' into table students;

inpath could be a directory. then all files in directory will be loaded.

SET hive.mapred.supports.subdirectories=true
SET mapred.input.dir.recursive=true;

# if you use match rule like this. then just files in second level path will be loaded.
load data local inpath '/home/ec2-user/jun/dataPath/*/*' into table  students;

## records.
select count(1) from cite;


## load data

hive -e 'select * from dp_test.test_change'

hive -f /home/dashuju/hive.sql

load data local inpath '/home/dashuju/clickstream/clickstream.log' INTO TABLE dp_ods.o_research_click_stream PARTITION(etl_date='2016-11-22')

## exec via -e.
 bin/hive -e "set mapreduce.job.queuename=root.etl; select count(1) from dp_ods.o_zhengxin_pbc_j_assets_new_s;"

 ## exec on spark
 set spark.home=/opt/apps/spark/spark-2.1.0-bin-hadoop2-without-hive;
 set hive.execution.engine=spark;
 set spark.job.queuename=root.etl;
 select count(1) from dp_ods.o_zhengxin_pbc_j_assets_new_s;
 