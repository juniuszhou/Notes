http://www.infoq.com/cn/articles/kafka-analysis-part-2

produce has both sync and async mode. 
if sync and three exception of send, then user can decide continue send following data
or resend.
if async, producer will log data that not sent successfully. so if no manual restore data
from log, the data will be lost.

leader elect. if no leader for each partition, then hard to keep consistency for each partition.
on each partition case, producer and consumer just interact with leader.
follower just sync data from leader.

# how to send message to broker
the producer at first find out leader from zookeeper.
and just send data to leader. 
leader write log. and each follower pull log from leader.
then follower send ACK to leader before write data into log. 
Then leader send ack to producer.

so there is rarely case, the follower ack the data but write data into disc unsucessfully.

# reader data procedure is the same. the offset committed data, means all follower get the 
log, then data expose to consumer.



# leader keep the replica list, ISR in-sync replica.
if a follower down or far fall behind, the leader remove this follower from list.

kafka just handle fail/recover, not Byzantine, which means some node create false data.

# algorithm for leader elect.
一种非常常用的选举leader的方式是“Majority Vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个Replica（包含Leader和Follower），那在commit之前必须保证有f+1个Replica复制完消息，为了保证正确选出新的Leader，fail的Replica不能超过f个。因为在剩下的任意f+1个Replica里，至少有一个Replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几个Broker，而非最慢那个。Majority Vote也有一些劣势，为了保证Leader Election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的Replica，如果要容忍2个Follower挂掉，必须要有5个以上的Replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在ZooKeeper这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA Feature是基于majority-vote-based journal，但是它的数据存储并没有使用这种方式。

实际上，Leader Election算法非常多，比如ZooKeeper的Zab, Raft和Viewstamped Replication。而Kafka所使用的Leader Election算法更像微软的PacificA算法。

Kafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个Replica的失败，Majority Vote和ISR在commit前需要等待的Replica数量是一样的，但是ISR需要的总的Replica的个数几乎是Majority Vote的一半。

虽然Majority Vote与ISR相比有不需等待最慢的Broker这一优势，但是Kafka作者认为Kafka可以通过Producer选择是否被commit阻塞来改善这一问题，并且节省下来的Replica和磁盘使得ISR模式仍然值得


#

