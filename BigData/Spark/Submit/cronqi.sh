export JAVA_HOME=/usr/lib/jvm/jre-1.8.0
export HADOOP_CONF_DIR=/usr/local/hadoop-2.6.3/etc/hadoop
export PATH=/usr/local/hadoop-2.6.3/bin:$PATH
export PATH=/usr/local/hadoop-2.6.3/sbin:$PATH
export PATH=/usr/local/spark-1.6.1-bin-hadoop2.6/bin:$PATH
export PATH=/usr/local/spark-1.6.1-bin-hadoop2.6/sbin:$PATH
export QILIB=hdfs://namenode01.bi.10101111.com:8020/user/qixing/share/lib
export QICONF=hdfs://namenode01.bi.10101111.com:8020/user/qixing/conf
export JUN=hdfs://namenode01.bi.10101111.com:8020/user/jzhou/sparkLib

USER="qixing"
HDFS_PATH="hdfs://namenode01.bi.10101111.com:8020"
SPARK_JAR="--conf spark.yarn.jar=$HDFS_PATH/user/$USER/share/lib/spark-assembly-1.6.1-hadoop2.6.0.jar"

#EXECUTOR_JVM='−−conf "spark.executor.extraJavaOptions=−XX:+UseParallelGC −XX:+UseParallelOldGC −XX:ParallelGCThreads=64 −XX:NewRatio=3 −XX:SurvivorRatio=3 −XX:+HeapDumpOnOutOfMemoryError −XX:HeapDumpPath=/home/spark/heapdump.bin −XX:+PrintHeapAtGC −XX:PermSize=256m −XX:MaxPermSize=256m  −XX:+PrintGCDetails −XX:+PrintGCTimeStamps −XX:+PrintGCDateStamps −XX:+PrintTenuringDistribution −XX:+PrintGCApplicationStoppedTime −XX:−OmitStackTraceInFastThrow  −verbose:gc −Xloggc:/tmp/spark.executor.gc.log "'
cd /usr/local/home/jzhou/jar
#/usr/local/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.ucar.growth.analysis.aggregation.hiveMemberJoin.HiveBigTableJoin --master yarn --deploy-mode cluster --executor-memory 12G --driver-memory 8G --driver-cores 4 --num-executors 120 --executor-cores 3 --conf "spark.executor.extraJavaOptions=-XX:MaxPermSize=2048m -XX:PermSize=2048m -verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps" --conf "spark.driver.extraJavaOptions=-XX:MaxPermSize=2048m -XX:PermSize=512m" --conf spark.shuffle.service.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.sql.tungsten.enabled=true --conf spark.driver.maxResultSize=8G --conf spark.akka.frameSize=2046  --conf spark.yarn.driver.memoryOverhead=2000 --conf spark.yarn.executor.memoryOverhead=10000 --conf spark.network.timeout=1200s --conf spark.shuffle.compress=true  --jars $QILIB/spark-listener-0.1.jar,$JUN/datanucleus-api-jdo-3.2.6.jar,$JUN/datanucleus-core-3.2.10.jar,$JUN/datanucleus-rdbms-3.2.9.jar,$JUN/guava-14.0.1.jar,$JUN/mysql-connector-java-5.1.38-bin.jar --conf spark.extraListeners=org.apache.spark.BIListener --files $QICONF/metrics.properties,../hive02/hive-site.xml --conf spark.metrics.conf=metrics.properties --driver-class-path $JUN/mysql-connector-java-5.1.38-bin.jar aggregation-growth-analysis-1.0-SNAPSHOT.jar 1000
#/usr/local/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.ucar.growth.analysis.aggregation.hiveMemberJoin.HiveBigTableJoin --master yarn --deploy-mode cluster --executor-memory 16G --driver-memory 8G --driver-cores 2 --num-executors 150 --executor-cores 2 --conf "spark.executor.extraJavaOptions=-XX:NewRatio=2 -XX:PermSize=128m -XX:MaxPermSize=128m -XX:SurvivorRatio=6  -verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps " --conf "spark.driver.extraJavaOptions=-XX:MaxPermSize=2048m -XX:PermSize=512m" --conf spark.shuffle.service.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.sql.tungsten.enabled=true --conf spark.driver.maxResultSize=8G --conf spark.akka.frameSize=2046  --conf spark.yarn.driver.memoryOverhead=2000 --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.network.timeout=300s --conf spark.shuffle.compress=true  --jars $QILIB/spark-listener-0.1.jar,$JUN/datanucleus-api-jdo-3.2.6.jar,$JUN/datanucleus-core-3.2.10.jar,$JUN/datanucleus-rdbms-3.2.9.jar,$JUN/guava-14.0.1.jar,$JUN/mysql-connector-java-5.1.38-bin.jar --conf spark.extraListeners=org.apache.spark.BIListener --files $QICONF/metrics.properties,../hive02/hive-site.xml --conf spark.metrics.conf=metrics.properties --driver-class-path $JUN/mysql-connector-java-5.1.38-bin.jar aggregation-growth-analysis-1.0-SNAPSHOT.jar 1000
# 3hour gc failed
#/usr/local/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.ucar.growth.analysis.aggregation.hiveMemberJoin.HiveBigTableJoin --master yarn --deploy-mode cluster --executor-memory 20G --driver-memory 8G --driver-cores 2 --num-executors 80 --executor-cores 3 --conf "spark.executor.extraJavaOptions=-XX:NewRatio=2 -XX:PermSize=128m -XX:MaxPermSize=128m -XX:SurvivorRatio=6  -verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps " --conf "spark.driver.extraJavaOptions=-XX:MaxPermSize=2048m -XX:PermSize=512m" --conf spark.shuffle.service.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.sql.tungsten.enabled=true --conf spark.driver.maxResultSize=8G --conf spark.akka.frameSize=2046  --conf spark.yarn.driver.memoryOverhead=2000 --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.network.timeout=300s --conf spark.shuffle.compress=true  --jars $QILIB/spark-listener-0.1.jar,$JUN/datanucleus-api-jdo-3.2.6.jar,$JUN/datanucleus-core-3.2.10.jar,$JUN/datanucleus-rdbms-3.2.9.jar,$JUN/guava-14.0.1.jar,$JUN/mysql-connector-java-5.1.38-bin.jar --conf spark.extraListeners=org.apache.spark.BIListener --files $QICONF/metrics.properties,../hive02/hive-site.xml --conf spark.metrics.conf=metrics.properties --driver-class-path $JUN/mysql-connector-java-5.1.38-bin.jar aggregation-growth-analysis-1.0-SNAPSHOT.jar 800
# failer 1** times map work with only 7 hosts
#-XX:+UseParallelGC  -XX:+UseParallelOldGC
#/usr/local/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.ucar.growth.analysis.aggregation.hiveMemberJoin.HiveBigTableJoin --master yarn --deploy-mode cluster --executor-memory 16G --driver-memory 8G --driver-cores 2 --num-executors 80 --executor-cores 4 --conf "spark.executor.extraJavaOptions=-XX:NewRatio=4 -XX:+UseParallelGC  -XX:+UseParallelOldGC  -XX:PermSize=128m -XX:MaxPermSize=128m -XX:SurvivorRatio=6  -verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps " --conf "spark.driver.extraJavaOptions=-XX:MaxPermSize=2048m -XX:PermSize=512m" --conf spark.shuffle.service.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.sql.tungsten.enabled=true --conf spark.driver.maxResultSize=8G --conf spark.akka.frameSize=2046  --conf spark.yarn.driver.memoryOverhead=2000 --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.network.timeout=600s --conf spark.shuffle.compress=true  --jars $QILIB/spark-listener-0.1.jar,$JUN/datanucleus-api-jdo-3.2.6.jar,$JUN/datanucleus-core-3.2.10.jar,$JUN/datanucleus-rdbms-3.2.9.jar,$JUN/guava-14.0.1.jar,$JUN/mysql-connector-java-5.1.38-bin.jar --conf spark.extraListeners=org.apache.spark.BIListener --files $QICONF/metrics.properties,../hive02/hive-site.xml --conf spark.metrics.conf=metrics.properties --driver-class-path $JUN/mysql-connector-java-5.1.38-bin.jar aggregation-growth-analysis-1.0-SNAPSHOT.jar 2000
#/usr/local/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.ucar.growth.analysis.aggregation.hiveMemberJoin.HiveBigTableJoin --master yarn --deploy-mode cluster --executor-memory 16G --driver-memory 8G --driver-cores 2 --num-executors 80 --executor-cores 4 --conf "spark.executor.extraJavaOptions=-XX:NewRatio=4 -XX:+UseParallelGC  -XX:+UseParallelOldGC  -XX:PermSize=128m -XX:MaxPermSize=128m -XX:SurvivorRatio=6  -verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps " --conf "spark.driver.extraJavaOptions=-XX:MaxPermSize=2048m -XX:PermSize=512m" --conf spark.shuffle.service.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.sql.tungsten.enabled=true --conf spark.driver.maxResultSize=8G --conf spark.akka.frameSize=2046  --conf spark.yarn.driver.memoryOverhead=2000 --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.network.timeout=600s --conf spark.shuffle.compress=true  --jars $QILIB/spark-listener-0.1.jar,$JUN/datanucleus-api-jdo-3.2.6.jar,$JUN/datanucleus-core-3.2.10.jar,$JUN/datanucleus-rdbms-3.2.9.jar,$JUN/guava-14.0.1.jar,$JUN/mysql-connector-java-5.1.38-bin.jar --conf spark.extraListeners=org.apache.spark.BIListener --files $QICONF/metrics.properties,../hive02/hive-site.xml --conf spark.metrics.conf=metrics.properties --driver-class-path $JUN/mysql-connector-java-5.1.38-bin.jar aggregation-growth-analysis-1.0-SNAPSHOT.jar 2200
#failed  1700G blockmanager
/usr/local/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.ucar.growth.analysis.aggregation.hiveMemberJoin.HiveBigTableJoin --master yarn --deploy-mode cluster --executor-memory 18G --driver-memory 8G --driver-cores 2 --num-executors 100 --executor-cores 2 --conf "spark.executor.extraJavaOptions=-XX:NewRatio=5 -XX:+UseParallelGC  -XX:+UseParallelOldGC  -XX:PermSize=128m -XX:MaxPermSize=128m -XX:SurvivorRatio=6  -verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps " --conf "spark.driver.extraJavaOptions=-XX:MaxPermSize=2048m -XX:PermSize=512m" --conf spark.shuffle.service.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.sql.tungsten.enabled=true --conf spark.driver.maxResultSize=8G --conf spark.akka.frameSize=2046  --conf spark.yarn.driver.memoryOverhead=2000 --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.network.timeout=900s --conf spark.shuffle.compress=true  --jars $QILIB/spark-listener-0.1.jar,$JUN/datanucleus-api-jdo-3.2.6.jar,$JUN/datanucleus-core-3.2.10.jar,$JUN/datanucleus-rdbms-3.2.9.jar,$JUN/guava-14.0.1.jar,$JUN/mysql-connector-java-5.1.38-bin.jar --conf spark.extraListeners=org.apache.spark.BIListener --files $QICONF/metrics.properties,../hive02/hive-site.xml --conf spark.metrics.conf=metrics.properties --driver-class-path $JUN/mysql-connector-java-5.1.38-bin.jar aggregation-growth-analysis-1.0-SNAPSHOT.jar 2002
