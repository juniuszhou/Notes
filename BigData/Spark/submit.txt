# Run application locally on 8 cores
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local[8] \
  /path/to/examples.jar \
  100

# Run on a Spark standalone cluster in client deploy mode
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000

# Run on a Spark standalone cluster in cluster deploy mode with supervise
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --deploy-mode cluster
  --supervise
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000

# Run on a YARN cluster
export HADOOP_CONF_DIR=XXX
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode cluster \  # can be client for client mode
  --executor-memory 20G \
  --num-executors 50 \
  /path/to/examples.jar \
  1000

# Run a Python application on a Spark standalone cluster
./bin/spark-submit \
  --master spark://207.184.161.138:7077 \
  examples/src/main/python/pi.py \
  1000

spark-submit --master yarn --deploy-mode client pi.py 100

# Run on a Mesos cluster in cluster deploy mode with supervise
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master mesos://207.184.161.138:7077 \
  --deploy-mode cluster
  --supervise
  --executor-memory 20G \
  --total-executor-cores 100 \
  http://path/to/examples.jar \
  1000
  


## define extra listener for spark job for metrics collection.
## put common lib in hdfs to reduce transfer file when submit.

export JAVA_HOME=/usr/lib/jvm/jre-1.8.0
export HADOOP_CONF_DIR=/usr/local/hadoop-2.6.3/etc/hadoop
export PATH=/usr/local/hadoop-2.6.3/bin:$PATH
export PATH=/usr/local/hadoop-2.6.3/sbin:$PATH
export PATH=/usr/local/spark-1.6.1-bin-hadoop2.6/bin:$PATH
export PATH=/usr/local/spark-1.6.1-bin-hadoop2.6/sbin:$PATH
export QILIB=hdfs://namenode01.bi.10101111.com:8020/user/qixing/share/lib
export QICONF=hdfs://namenode01.bi.10101111.com:8020/user/qixing/conf
export JUN=hdfs://namenode01.bi.10101111.com:8020/user/jzhou/sparkLib
cd /usr/local/home/jzhou/jar
/usr/local/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.ucar.growth.analysis.aggregation.hiveMemberJo
in.HiveJoinForTag --master yarn --deploy-mode cluster --executor-memory 18G --driver-memory 18G --driver-cores 
4 --num-executors 50 --executor-cores 1 --conf "spark.executor.extraJavaOptions=-XX:MaxPermSize=2048m -XX:PermS
ize=512m" --conf "spark.driver.extraJavaOptions=-XX:MaxPermSize=2048m -XX:PermSize=512m" --conf spark.shuffle.s
ervice.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.sql.tungsten.enabled=true --conf spa
rk.driver.maxResultSize=8G --conf spark.akka.frameSize=2046  --conf spark.yarn.driver.memoryOverhead=10000 --co
nf spark.yarn.executor.memoryOverhead=10000 --conf spark.network.timeout=1200s --conf spark.shuffle.compress=tr
ue  --jars $QILIB/spark-listener-0.1.jar,$JUN/datanucleus-api-jdo-3.2.6.jar,$JUN/datanucleus-core-3.2.10.jar,$J
UN/datanucleus-rdbms-3.2.9.jar,$JUN/guava-14.0.1.jar,$JUN/mysql-connector-java-5.1.38-bin.jar --conf spark.extr
aListeners=org.apache.spark.BIListener --files $QICONF/metrics.properties,../data/hive-site.xml --conf spark.me
trics.conf=metrics.properties --driver-class-path $JUN/mysql-connector-java-5.1.38-bin.jar aggregation-growth-a
nalysis-1.0-SNAPSHOT.jar 1000


[jzhou@xxzhang01.intranet.prod.bj1 home]$ hadoop fs -ls /user/qixing/share/lib
Found 3 items
-rw-r--r--   3 qixing supergroup     983914 2016-09-09 16:24 /user/qixing/share/lib/mysql-connector-java-5.1.38-bin.jar
-rw-r--r--   3 qixing supergroup  187698038 2016-09-09 16:24 /user/qixing/share/lib/spark-assembly-1.6.1-hadoop2.6.0.jar
-rw-r--r--   3 qixing supergroup       7782 2016-09-09 16:24 /user/qixing/share/lib/spark-listener-0.1.jar
[jzhou@xxzhang01.intranet.prod.bj1 home]$ hadoop fs -ls /user/qixing/conf
Found 1 items
-rw-r--r--   3 qixing supergroup       6921 2016-09-19 15:32 /user/qixing/conf/metrics.properties



######## SUBMIT 2.0
## there is no assembly jar for spark 2.0 since.
so you must set SPARK_HOME. then submit will use jars under SPARK_HOME
as dependent jars send to distributed environment for job running.


bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client --archives hdfs://10.100.12.12:8020/user/dashuju/spark_archive.jar examples/jars/spark-examples_2.11-2.1.0.jar 1000



spark.yarn.archive

spark.yarn.jars=