# 统计独立
独立就是两个随机变量相互独立，等价于f(x,y)=g(x)h(y)，即联合密度函数等于两个边缘密度的乘积。对于离散的随机变量会稍有不同，Pr(X=x,Y=y)=Pr(X=x)Pr(Y=y) for all x and y。

# 统计相关
相关指的是线性关系 不相关即是没有线性关系。
相关系数是研究变量之间线性相关程度的量



mcnemar
friedman
nemenyi


covariance 协方差，用来描述二个随机变量之间的相关度。
使用各自变量和期望的差的乘积的期望来计算。
我们可以使用同向偏度来解释它。它的取值可以是正，负。
如果二个随机变量大多同时大于或者小于期望，那么协方差为正。
反之为负。

由于协方差可以随着变量乘以一个因子而改变。所以不能准确的
度量相关性的大小。这时我们引入了相关系数。
即归一化的协方差。计算的时候各自除以自己的方差。
这样协方差会在-1和1之间。

随机变量独立那么肯定不相关，反之不一定成立。

